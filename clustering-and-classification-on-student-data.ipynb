{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "**Clustering and Multiclass Classification (Predictive Modeling / Machine Learning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "**INTRODUCTION:**\n",
    "\n",
    "STG (The degree of study time for goal object materials)\n",
    "<br>\n",
    "SCG (The degree of repetition number of user for goal object materials)\n",
    "<br>\n",
    "STR (The degree of study time of user for related objects with goal object)\n",
    "<br>\n",
    "LPR (The exam performance of user for related objects with goal object)\n",
    "<br>\n",
    "PEG (The exam performance of user for goal objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the excel file and look at column names\n",
    "os.chdir(\"/kaggle/input\")\n",
    "orig = pd.read_csv('user-knowledge/User Knowledge.csv')\n",
    "orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns containing the data about student's knowledge\n",
    "knowledge = orig.iloc[:,:5]\n",
    "knowledge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of the featuers to visualize the data\n",
    "knowledge.hist(bins=50, figsize = (8,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we will explore K-Means clustering and look closely at the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-Means Clustering with values of k from 1 to 10 and plot k v/s Within Cluster Sum of Squares\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=400, n_init=20, random_state=0)\n",
    "    kmeans.fit(knowledge)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=400, n_init=20, random_state=0)\n",
    "kmeans.fit(knowledge)\n",
    "k_class = kmeans.predict(knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA and filtering 3 principal components for data visualization\n",
    "pca = PCA(n_components=3)\n",
    "principalComponents = pca.fit_transform(knowledge)\n",
    "PDF = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2', 'PC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column 'Class' to the data sets\n",
    "PDF.loc[:, 'Cluster'] = pd.Series(k_class)\n",
    "knowledge_class = knowledge.copy()\n",
    "knowledge_class['Class'] = k_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of points in each cluster\n",
    "PDF['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a color to each cluster\n",
    "PDF['Color'] = PDF['Cluster'].map({0 : 'red', 1 : 'blue', 2 : 'green'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 2 principal components and color by cluster\n",
    "a1 = PDF['PC1']\n",
    "a2 = PDF['PC2']\n",
    "a3 = PDF['PC3']\n",
    "c1 = PDF['Color']\n",
    "plt.scatter(a1, a2, c = c1, alpha=0.3, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-D plot of the data using 3 principal components\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(a1, a2, a3, alpha = 0.4, c = c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at how to the 3 classes differ by calculating their averages on each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_class.groupby(['Class']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KNN (K-Nearest Neighbors) - KNN uses distance as the metric and the labels for the dataset were also obtained using distance as the metric when we applied K-Means Clustering. Thus, KNN may perform well on this dataset.\n",
    "* Decision Tree Classifier - We almost always want to apply a few Machine Learning methods to any dataset and compare them based on a suitable evaluation metric rather than selecting one final model based only on intusion. Although decision tess may not perform best on a small data such as this one, they are highly interpretable.\n",
    "* Naive Bayes - Based on assumption that variables are independent and making a probabilistic estimation using  amaximum likelihood hypothesis, this algorithm is highly efficient as compared to other Machine Lerning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slipt the data into train and test data sets\n",
    "X = knowledge_class.iloc[:, :-1]\n",
    "Y = knowledge_class.iloc[:, -1]\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN for various values of k and plot of k v/s accuracy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "accuracy = []\n",
    "for i in range(1,12):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i).fit(xTrain, yTrain)\n",
    "    accuracy.append(knn.score(xTest, yTest))\n",
    "\n",
    "plt.plot(range(1,12), accuracy)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy') \n",
    "plt.title('k v/s Accuracy for KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN model and evaluation for optimal value of k (8 in this case)\n",
    "knn = KNeighborsClassifier(n_neighbors = accuracy.index(max(accuracy))+1).fit(xTrain, yTrain)\n",
    "knn_predictions = knn.predict(xTest)\n",
    "knn_accuracy = knn.score(xTest, yTest)\n",
    "knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_CM = confusion_matrix(yTest, knn_predictions) # KNN Confusion Matrix\n",
    "knn_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier and evaluation for optimal value of k\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(xTrain, yTrain) \n",
    "dtree_predictions = dtree_model.predict(xTest)\n",
    "dt_accuracy = dtree_model.score(xTest, yTest)\n",
    "dt_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_CM = confusion_matrix(yTest, dtree_predictions) # Decision Tree confusion Matrix\n",
    "DT_CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes model and evaluation for optimal value of k\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(xTrain, yTrain)\n",
    "gnb_predictions = gnb.predict(xTest)\n",
    "gnb_accuracy = gnb.score(xTest, yTest)\n",
    "gnb_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CM = confusion_matrix(yTest, gnb_predictions) # Naive Bayes confusion Matrix\n",
    "NB_CM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude  that the Naive Bayes classifier performed better than KNN and Decision Tree classifier based on the results of accuracy as can be verified by comparing the confusion matrices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
